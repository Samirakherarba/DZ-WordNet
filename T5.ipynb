{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " T5 google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faycal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/mt5-large')\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/mt5-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'translation': './dataset_for_training.csv'}, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mot en fr': 'saisir', 'Daridja arabe': 'حكم'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['translation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    source_tokens = tokenizer.encode_plus(\n",
    "        example[\"Mot en fr\"],  \n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    target_tokens = tokenizer.encode_plus(\n",
    "        example[\"Daridja arabe\"], \n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": source_tokens[\"input_ids\"].flatten(),\n",
    "        \"attention_mask\": source_tokens[\"attention_mask\"].flatten(),\n",
    "        \"labels\": target_tokens[\"input_ids\"].flatten()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  327, 45144,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  259, 18197,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"translation\"][0]  \n",
    "tokenized_example = tokenize_function(example)\n",
    "print(tokenized_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset[\"translation\"].filter(lambda x: x[\"Mot en fr\"] is not None and x[\"Daridja arabe\"] is not None)\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Mot en fr', 'Daridja arabe', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14954\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Mot en fr', 'Daridja arabe', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3739\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in labels_str], pred_str)\n",
    "\n",
    "    return {\"bleu_score\": bleu_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_checkpoint_dir = \"./Modal1\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Modal1\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=500,  \n",
    "    overwrite_output_dir=False,  \n",
    "    resume_from_checkpoint=resume_checkpoint_dir  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab14518f593e4e56879e8f9453eba93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6189, 'grad_norm': 2.1504368782043457, 'learning_rate': 4.8663101604278076e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7574, 'grad_norm': 0.910918653011322, 'learning_rate': 4.732620320855615e-05, 'epoch': 0.53}\n",
      "{'loss': 0.681, 'grad_norm': 0.7778192162513733, 'learning_rate': 4.598930481283423e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6446, 'grad_norm': 0.9133714437484741, 'learning_rate': 4.4652406417112304e-05, 'epoch': 1.07}\n",
      "{'loss': 0.5912, 'grad_norm': 1.6034926176071167, 'learning_rate': 4.331550802139038e-05, 'epoch': 1.34}\n",
      "{'loss': 0.5784, 'grad_norm': 0.7972973585128784, 'learning_rate': 4.197860962566845e-05, 'epoch': 1.6}\n",
      "{'loss': 0.5728, 'grad_norm': 1.0540673732757568, 'learning_rate': 4.0641711229946525e-05, 'epoch': 1.87}\n",
      "{'loss': 0.5353, 'grad_norm': 1.4703896045684814, 'learning_rate': 3.93048128342246e-05, 'epoch': 2.14}\n",
      "{'loss': 0.5068, 'grad_norm': 1.052030086517334, 'learning_rate': 3.796791443850268e-05, 'epoch': 2.41}\n",
      "{'loss': 0.5069, 'grad_norm': 1.119246006011963, 'learning_rate': 3.6631016042780753e-05, 'epoch': 2.67}\n",
      "{'loss': 0.5034, 'grad_norm': 0.73586505651474, 'learning_rate': 3.529411764705883e-05, 'epoch': 2.94}\n",
      "{'loss': 0.4687, 'grad_norm': 0.8226745128631592, 'learning_rate': 3.39572192513369e-05, 'epoch': 3.21}\n",
      "{'loss': 0.4591, 'grad_norm': 0.8904627561569214, 'learning_rate': 3.2620320855614975e-05, 'epoch': 3.48}\n",
      "{'loss': 0.4589, 'grad_norm': 0.7578887939453125, 'learning_rate': 3.128342245989305e-05, 'epoch': 3.74}\n",
      "{'loss': 0.4498, 'grad_norm': 0.8590931296348572, 'learning_rate': 2.9946524064171122e-05, 'epoch': 4.01}\n",
      "{'loss': 0.4155, 'grad_norm': 0.8416089415550232, 'learning_rate': 2.8609625668449196e-05, 'epoch': 4.28}\n",
      "{'loss': 0.4173, 'grad_norm': 0.7122125029563904, 'learning_rate': 2.7272727272727273e-05, 'epoch': 4.55}\n",
      "{'loss': 0.4144, 'grad_norm': 0.7922715544700623, 'learning_rate': 2.5935828877005347e-05, 'epoch': 4.81}\n",
      "{'loss': 0.4128, 'grad_norm': 1.4970874786376953, 'learning_rate': 2.4598930481283424e-05, 'epoch': 5.08}\n",
      "{'loss': 0.3862, 'grad_norm': 0.9522221088409424, 'learning_rate': 2.32620320855615e-05, 'epoch': 5.35}\n",
      "{'loss': 0.3889, 'grad_norm': 0.758745551109314, 'learning_rate': 2.192513368983957e-05, 'epoch': 5.61}\n",
      "{'loss': 0.3787, 'grad_norm': 0.761408805847168, 'learning_rate': 2.058823529411765e-05, 'epoch': 5.88}\n",
      "{'loss': 0.3762, 'grad_norm': 0.9881584644317627, 'learning_rate': 1.9251336898395722e-05, 'epoch': 6.15}\n",
      "{'loss': 0.3551, 'grad_norm': 0.6850549578666687, 'learning_rate': 1.7914438502673796e-05, 'epoch': 6.42}\n",
      "{'loss': 0.3639, 'grad_norm': 1.1036587953567505, 'learning_rate': 1.6577540106951873e-05, 'epoch': 6.68}\n",
      "{'loss': 0.3615, 'grad_norm': 1.33849036693573, 'learning_rate': 1.5240641711229947e-05, 'epoch': 6.95}\n",
      "{'loss': 0.3397, 'grad_norm': 1.690130352973938, 'learning_rate': 1.3903743315508022e-05, 'epoch': 7.22}\n",
      "{'loss': 0.3454, 'grad_norm': 1.0579711198806763, 'learning_rate': 1.2566844919786098e-05, 'epoch': 7.49}\n",
      "{'loss': 0.341, 'grad_norm': 1.0972784757614136, 'learning_rate': 1.1229946524064172e-05, 'epoch': 7.75}\n",
      "{'loss': 0.3458, 'grad_norm': 0.9292057752609253, 'learning_rate': 9.893048128342247e-06, 'epoch': 8.02}\n",
      "{'loss': 0.3244, 'grad_norm': 0.9827554821968079, 'learning_rate': 8.556149732620321e-06, 'epoch': 8.29}\n",
      "{'loss': 0.3274, 'grad_norm': 1.4099150896072388, 'learning_rate': 7.2192513368983955e-06, 'epoch': 8.56}\n",
      "{'loss': 0.3267, 'grad_norm': 1.0601311922073364, 'learning_rate': 5.882352941176471e-06, 'epoch': 8.82}\n",
      "{'loss': 0.325, 'grad_norm': 0.8129891157150269, 'learning_rate': 4.5454545454545455e-06, 'epoch': 9.09}\n",
      "{'loss': 0.3234, 'grad_norm': 0.862271249294281, 'learning_rate': 3.208556149732621e-06, 'epoch': 9.36}\n",
      "{'loss': 0.3126, 'grad_norm': 0.8069100379943848, 'learning_rate': 1.8716577540106951e-06, 'epoch': 9.63}\n",
      "{'loss': 0.318, 'grad_norm': 0.9965262413024902, 'learning_rate': 5.347593582887701e-07, 'epoch': 9.89}\n",
      "{'train_runtime': 133181.6806, 'train_samples_per_second': 1.123, 'train_steps_per_second': 0.14, 'train_loss': 0.6244368425665054, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18700, training_loss=0.6244368425665054, metrics={'train_runtime': 133181.6806, 'train_samples_per_second': 1.123, 'train_steps_per_second': 0.14, 'total_flos': 1.746864549888e+16, 'train_loss': 0.6244368425665054, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "checkpoint_dir = \"./Modal1/checkpoint-18500\"\n",
    "\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint_dir)\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/mt5-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tokenized_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104cc4a8cb4b423f8a4036d37a453b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1194195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.to_csv(\"test_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def generate_predictions(model, test_dataset, tokenizer):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for example in tqdm(test_dataset):\n",
    "        if isinstance(example[\"input_ids\"], list):\n",
    "            input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
    "            attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(model.device)\n",
    "        else:\n",
    "            input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "            attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=20, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        reference = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\faycal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\faycal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in c:\\users\\faycal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\faycal\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 682.7 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpus_bleu\n\u001b[1;32m----> 3\u001b[0m predictions, references \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m corpus_bleu([[ref] \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m references], predictions)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bleu_score)\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[1;34m(model, test_dataset, tokenizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m references \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(test_dataset):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m      8\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "predictions, references = generate_predictions(model, test_dataset, tokenizer)\n",
    "bleu_score = corpus_bleu([[ref] for ref in references], predictions)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3739 [00:00<?, ?it/s]c:\\Users\\Faycal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3739/3739 [41:54<00:00,  1.49it/s] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions, references\n\u001b[0;32m     27\u001b[0m predictions, references \u001b[38;5;241m=\u001b[39m generate_predictions(model, test_dataset, tokenizer)\n\u001b[1;32m---> 28\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bleu_score)\n",
      "File \u001b[1;32mc:\\Users\\Faycal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\Faycal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[0;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Assuming generate_predictions, model, test_dataset, and tokenizer are defined elsewhere\n",
    "def generate_predictions(model, test_dataset, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in tqdm(test_dataset):\n",
    "        if isinstance(example[\"input_ids\"], list):\n",
    "            input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
    "        else:\n",
    "            input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            reference = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(prediction.split())\n",
    "        references.append(reference.split())\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "predictions, references = generate_predictions(model, test_dataset, tokenizer)\n",
    "bleu_score = corpus_bleu([[ref] for ref in references], predictions)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (French): Amina sois gentille s'il te plait\n",
      "Translated (Darija): امين كون لطيف\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "\n",
    "def generate_translation(input_text):\n",
    "    input_text = \"translate French to Darija: \" + input_text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "input_text = \"Amina sois gentille s'il te plait\"\n",
    "translated_text = generate_translation(input_text)\n",
    "print(\"Input (French):\", input_text)\n",
    "print(\"Translated (Darija):\", translated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
